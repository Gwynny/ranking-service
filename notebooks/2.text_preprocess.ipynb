{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook I will show how I will preprocess text and transform it to Embedding Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glue_df(glue_df) -> pd.DataFrame:\n",
    "    glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "    glue_df_fin = pd.DataFrame({\n",
    "        'id_left': glue_df['qid1'],\n",
    "        'id_right': glue_df['qid2'],\n",
    "        'text_left': glue_df['question1'],\n",
    "        'text_right': glue_df['question2'],\n",
    "        'label': glue_df['is_duplicate'].astype(int)\n",
    "    })\n",
    "    return glue_df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join('', os.pardir))\n",
    "train_df = pd.read_csv(parent_dir + '/data/raw/QQP/train.tsv', sep='\\t')\n",
    "train_df = get_glue_df(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step is punctuation and other unnecessary symbols deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_punctuation(inp_str: str) -> str:\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    new_str = inp_str.translate(translator)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return None'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_punctuation('return!None')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second step is lowering and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete punctuation -> lower -> tokenize\n",
    "def simple_preproc(inp_str: str) -> List[str]:\n",
    "    no_punctuation_str = handle_punctuation(inp_str)\n",
    "    lowered_str = no_punctuation_str.lower()\n",
    "    splitted_doc = nltk.word_tokenize(lowered_str)\n",
    "    return splitted_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['return', 'none']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_preproc('return!None')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Third step is filter our words with lower occurences and create list with all tokens for creation of Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_rare_words(vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "    filtered_vocab = {x: count for x, count in vocab.items() if count >= min_occurancies}\n",
    "    return filtered_vocab\n",
    "\n",
    "def get_all_tokens(list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "    preped_series = []\n",
    "    for df in list_of_df:\n",
    "        preped_question1 = df['text_left'].apply(simple_preproc)\n",
    "        preped_question2 = df['text_right'].apply(simple_preproc)\n",
    "        preped_series.append(preped_question1)\n",
    "        preped_series.append(preped_question2)\n",
    "\n",
    "    concat_series = pd.concat(preped_series)\n",
    "    one_list_of_tokens = list(itertools.chain.from_iterable(concat_series.to_list()))\n",
    "    vocab = dict(Counter(one_list_of_tokens))\n",
    "    vocab = _filter_rare_words(vocab, min_occurancies)\n",
    "    return list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 58.7 s\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_tokens = get_all_tokens([train_df], min_occurancies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'is', 'the', 'life', 'of', 'a', 'math', 'student', 'could', 'you']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next step is creating Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_glove_embeddings(file_path: str) -> Dict[str, List[str]]:\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        glove_dict = {}\n",
    "        for line in file:\n",
    "            splitted_line = line.split()\n",
    "            word, embedding = splitted_line[0], splitted_line[1:]\n",
    "            glove_dict[word] = embedding\n",
    "    return glove_dict\n",
    "    \n",
    "def create_glove_emb_from_file(file_path: str, inner_keys: List[str],\n",
    "                               random_seed: int, rand_uni_bound: float\n",
    "                               ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "    np.random.seed(random_seed)\n",
    "    glove_dict = _read_glove_embeddings(file_path)\n",
    "    emb_dim = len(glove_dict['the'])\n",
    "    \n",
    "    emb_matrix = []\n",
    "    pad_vec = np.random.uniform(low=-rand_uni_bound, high=rand_uni_bound, size=emb_dim)\n",
    "    oov_vec = np.random.uniform(low=-rand_uni_bound, high=rand_uni_bound, size=emb_dim)\n",
    "    emb_matrix.append(pad_vec)\n",
    "    emb_matrix.append(oov_vec)\n",
    "    \n",
    "    vocab = {}\n",
    "    unk_words = []\n",
    "    vocab['PAD'], vocab['OOV'] = 0, 1\n",
    "    for ind, token in enumerate(inner_keys, 2):\n",
    "        if token in glove_dict.keys():\n",
    "            emb_matrix.append(glove_dict[token])\n",
    "            vocab[token] = ind\n",
    "        else:\n",
    "            unk_words.append(token)\n",
    "            vocab[token] = ind\n",
    "            random_emb = np.random.uniform(low=-rand_uni_bound, high=rand_uni_bound, size=emb_dim)\n",
    "            emb_matrix.append(random_emb)\n",
    "    emb_matrix = np.array(emb_matrix).astype(float)\n",
    "    return (emb_matrix, vocab, unk_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We retrieve pretrained Glove vectors and in case we didn't find word we replace it with uniform vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.41 s\n",
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "emb_matrix, vocab, unk_words = create_glove_emb_from_file(\n",
    "    parent_dir + '/data/raw/glove.6B.50d.txt', all_tokens, 0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24217, 83203, 83205)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we add two special symbols\n",
    "len(unk_words), len(all_tokens), len(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2910592166147855"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of unknown words\n",
    "len(unk_words) / len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(83205, 50, padding_idx=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying Torch API\n",
    "emb_matrix = torch.nn.Embedding.from_pretrained(torch.FloatTensor(emb_matrix), freeze=True, padding_idx=0)\n",
    "emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve 2 docs with 3 words example\n",
    "# this is how it will be used in model\n",
    "indices = torch.LongTensor([\n",
    "    [1, 33, 2],\n",
    "    [2, 4, 3]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix(indices).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92842f9dd492c40e6b909af2db934e37cdd29a62e6ee90ddfc29a823a33fb818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
